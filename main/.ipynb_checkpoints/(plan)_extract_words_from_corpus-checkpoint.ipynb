{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PreProcessing a text file**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "global variables:\n",
    "\n",
    "# definition: am_and_thanh_key = âm_without_thanh + thanh_num \n",
    "\n",
    "-, <dict> am_and_thanh_key_mapto_most_occurrence: mapping every am_and_thanh_key to the Standard CHOSEN word for that key.\n",
    "-, <dict> am_and_thanh_key_mapto_list_of_variation: mapping every am_and_thanh_key to a list of variation of that key found in text.\n",
    "\n",
    "-, <dict>: thanh_to_number: mapping a thanh to a specific number, eg: 'hoi' = 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pseudo-code for Preprocessing Text File:\n",
    "\n",
    "I, Text Normalization (Tokenization):\n",
    "SOLUTION:\n",
    "def create_new_processed_tf(file_path):\n",
    "    \"\"\"\n",
    "    GOAL: \n",
    "    1) Preprocess a text file (and save it \n",
    "    into a new file) for later steps.\n",
    "    input: raw text file (string)\n",
    "    returns: path of new processed text file (string)\n",
    "    \"\"\"\n",
    "    open read_file AND write_file as r_f, w_f:\n",
    "        for line in r_f and w_f:\n",
    "        # STEP 1: ÂM PROCESSING\n",
    "            TODO(1): replace case-folding (viet tat) with normal format\n",
    "            TODO(2): lowercase all letter chars.\n",
    "            TODO(3): sub special chars with \\w, then sub number with \\w\n",
    "            write modified line to a new file (TESTING PURPOSE)\n",
    "        return path_new_file\n",
    "        \n",
    "def text_normalization(file_path):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "-- TF-IDF and Word Count Visualization --\n",
    "def compute_tf_idf(path_processed_file):\n",
    "    \"\"\"\n",
    "    GOAL: Determine a list of stop-words from\n",
    "    the processed corpus.\n",
    "    input: string (path of text-normalized file)\n",
    "    return: list (of string)\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    process line-by-line in the given text file\n",
    "    am_split = split am of line\n",
    "    STRIP \\n from last char of line, then add it as last elem of am_split\n",
    "\n",
    "def stop_word_visualization(list_sw, path_processed_file):\n",
    "    \"\"\"\n",
    "    GOAL: count the # of occurence for \n",
    "    each stop word in the given file, then plot:\n",
    "    1) each stop word's ocurrence order by DESC frequency\n",
    "    2) percentage in the whole file w\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def normalize_am_with_thanh():\n",
    "    \"\"\"\n",
    "    GOAL: \n",
    "    \n",
    "    Key: \n",
    "    Value: \n",
    "    convert âm_with_thanh -> am_and_thanh_key\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def translate_function():\n",
    "    \"\"\"\n",
    "    GOAL: translate am_and_thanh_key back to normal âm_with_dấu \n",
    "    using am_and_thanh_key_mapto_most_occurrence dict\n",
    "    eg: thuy2 -> thuỷ \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF Steps**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = '/Users/trungdoquoc/Desktop/Personal Projects/VNese_assistant/VNese_Corpus/small_corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "with open(corpus_path) as txt:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a')\n",
      "('is', 'a', 'foo')\n",
      "('a', 'foo', 'bar')\n",
      "('foo', 'bar', 'sentences')\n",
      "('bar', 'sentences', 'and')\n",
      "('sentences', 'and', 'i')\n",
      "('and', 'i', 'want')\n",
      "('i', 'want', 'to')\n",
      "('want', 'to', 'ngramize')\n",
      "('to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "\n",
    "n = 3\n",
    "sixgrams = ngrams(sentence.split(), n)\n",
    "\n",
    "for grams in sixgrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(sentence):\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
