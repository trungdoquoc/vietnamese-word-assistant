{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "corpus_path = '/Users/trungdoquoc/Desktop/Personal Projects/VNese_assistant/VNese_Corpus/small_corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "nguyen_am_don = ['a', 'ă', 'â', 'e', 'ê', 'i', 'u', 'ư', 'o', 'ơ', 'ô', 'y']\n",
    "phu_am_db = ['gi', 'qu']\n",
    "\n",
    "thanh_dict = {\n",
    "    'sac': ['á', 'ắ', 'ấ', 'é', 'ế', 'í', 'ú', 'ứ', 'ó', 'ớ', 'ố', 'ý'],\n",
    "    'hoi': ['ả', 'ẳ', 'ẩ', 'ẻ', 'ể', 'ỉ', 'ủ', 'ử', 'ỏ', 'ở', 'ổ', 'ỷ'],\n",
    "    'nga': ['ã', 'ẵ', 'ẫ', 'ẽ', 'ễ', 'ĩ', 'ũ', 'ữ', 'õ', 'ỡ', 'ỗ', 'ỹ'],\n",
    "    'nang': ['ạ', 'ặ', 'ậ', 'ẹ', 'ệ', 'ị', 'ụ', 'ự', 'ọ', 'ợ', 'ộ', 'ỵ'],\n",
    "    'huyen': ['à', 'ằ', 'ầ', 'è', 'ề', 'ì', 'ù', 'ừ', 'ò', 'ờ', 'ồ', 'ỳ'],\n",
    "    'ngang': ['a', 'ă', 'â', 'e', 'ê', 'i', 'u', 'ư', 'o', 'ơ', 'ô', 'y']\n",
    "}\n",
    "\n",
    "bang_trac_dict = {\n",
    "    'T': ['sac', 'hoi', 'nga', 'nang'],\n",
    "    'B': ['huyen', 'ngang']\n",
    "}\n",
    "\n",
    "thanh_nguyen_am = {\n",
    "    'a': ['á', 'ả', 'ã', 'ạ', 'à', 'a'],\n",
    "    'ă': ['ắ', 'ẳ', 'ẵ', 'ặ', 'ằ', 'ă'],\n",
    "    'â': ['ấ', 'ẩ', 'ẫ', 'ậ', 'ầ', 'â'],\n",
    "    'e': ['é', 'ẻ', 'ẽ', 'ẹ', 'è', 'e'],\n",
    "    'ê': ['ế', 'ể', 'ễ', 'ệ', 'ề', 'ê'],\n",
    "    'i': ['í', 'ỉ', 'ĩ', 'ị', 'ì', 'i'],\n",
    "    'u': ['ú', 'ủ', 'ũ', 'ụ', 'ù', 'u'],\n",
    "    'ư': ['ứ', 'ử', 'ữ', 'ự', 'ừ', 'ư'],\n",
    "    'o': ['ó', 'ỏ', 'õ', 'ọ', 'ò', 'o'],\n",
    "    'ơ': ['ớ', 'ở', 'õ', 'ợ', 'ờ', 'ơ'],\n",
    "    'ô': ['ố', 'ổ', 'ỗ', 'ộ', 'ồ', 'ô'],\n",
    "    'y': ['ý', 'ỷ', 'ỹ', 'ỵ', 'ỳ', 'y']\n",
    "}\n",
    "\n",
    "###\n",
    "\n",
    "t1_vertical = ['a', 'ă', 'â', 'e', 'ê', 'i', 'o', 'ô', 'ơ', 'u', 'ư']\n",
    "t1_horizontal = ['a', 'c', 'ch', 'e/ê', 'i', 'm', 'n', 'ng', 'nh', 'o/ơ', 'p', 't', 'u', 'y']\n",
    "t1_matrix = [[0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "             [0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0],\n",
    "             [0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1],\n",
    "             [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0],\n",
    "             [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
    "             [1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
    "             [1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0],\n",
    "             [0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0],\n",
    "             [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0],\n",
    "             [1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1],\n",
    "             [1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0]]\n",
    "\n",
    "t2_vertical = ['iê', 'oa', 'oă', 'uâ', 'uô', 'uya', 'uyê', 'ươ', 'yê']\n",
    "t2_horizontal = ['a', 'c', 'ch', 'i', 'm', 'n', 'ng', 'nh', 'p', 't', 'u', 'y']\n",
    "t2_matrix = [[0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0],\n",
    "             [0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1],\n",
    "             [0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
    "             [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "             [0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0],\n",
    "             [1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
    "             [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "             [0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0],\n",
    "             [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0]]\n",
    "\n",
    "t1_complete = []\n",
    "for r in range(len(t1_matrix)):\n",
    "    row = []\n",
    "    for c in range(len(t1_matrix[r])):\n",
    "        if t1_matrix[r][c] == 1:\n",
    "            row.append(t1_vertical[r] + t1_horizontal[c])\n",
    "        else:\n",
    "            row.append('')\n",
    "    t1_complete.append(row)\n",
    "\n",
    "### Fix some special cases:\n",
    "t1_complete[6][3] = 'oe'\n",
    "t1_complete[9][3] = 'uê'\n",
    "t1_complete[0][9] = 'ao'\n",
    "t1_complete[3][9] = 'eo'\n",
    "t1_complete[9][9] = 'uơ'\n",
    "\n",
    "t2_complete = []\n",
    "for r in range(len(t2_matrix)):\n",
    "    row = []\n",
    "    for c in range(len(t2_matrix[r])):\n",
    "        if t2_matrix[r][c] == 1:\n",
    "            row.append(t2_vertical[r] + t2_horizontal[c])\n",
    "        else:\n",
    "            row.append('')\n",
    "    t2_complete.append(row)\n",
    "\n",
    "t2_complete[5][0] = 'uya'\n",
    "t2_complete[5][2] = 'uych'\n",
    "t2_complete[5][7] = 'uynh'\n",
    "t2_complete[5][9] = 'uyt'\n",
    "\n",
    "nguyen_am_da = []\n",
    "\n",
    "for r in t1_complete:\n",
    "    for e in r:\n",
    "        if e != '':\n",
    "            nguyen_am_da.append(e)\n",
    "            \n",
    "for r in t2_complete:\n",
    "    for e in r:\n",
    "        if e != '':\n",
    "            nguyen_am_da.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def replace_thanh_of_tieng(list_tieng_w_thanh):\n",
    "    \"\"\"\n",
    "    For a list of tieng, detect the letter with thanh \n",
    "    in each tiếng element, then replace it with that l\n",
    "    etter without thanh.\n",
    "    input: a list of tiếng in a word\n",
    "    returns: a list of tiếng, each tiếng is without thanh\n",
    "    \"\"\"\n",
    "    list_tieng_wo_thanh = [''] * len(list_tieng_w_thanh)\n",
    "    \n",
    "    for t in range(len(list_tieng_w_thanh)):\n",
    "        tieng = list_tieng_w_thanh[t]\n",
    "        for c in tieng:\n",
    "            for k in thanh_nguyen_am:\n",
    "                if c in thanh_nguyen_am[k]:\n",
    "                    tieng = tieng.replace(c, k)\n",
    "        list_tieng_wo_thanh[t] = tieng\n",
    "    return list_tieng_wo_thanh\n",
    "\n",
    "def detect_van(tieng):\n",
    "    \"\"\"\n",
    "    Detect all possible vần in **tieng_ko_dau**\n",
    "    (both complete and partial vần)\n",
    "    input: a string\n",
    "    returns: a list of string\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    longest_v = ''\n",
    "    \n",
    "    # special case: van = gi:\n",
    "    # ng. am don:\n",
    "    if tieng[:2] == 'gi':\n",
    "        if tieng[2:] == '':\n",
    "            res.append('i')\n",
    "        elif tieng[2:] in nguyen_am_don or tieng[2:] in nguyen_am_da:\n",
    "            res.append(tieng[2:])\n",
    "        else:\n",
    "            res.append(tieng[1:])\n",
    "        return res\n",
    "        \n",
    "    # special case: van = qu:\n",
    "    if tieng[0] == 'q':\n",
    "        if tieng[1:] in nguyen_am_da:\n",
    "            res.append(tieng[1:])\n",
    "        elif 'o' + tieng[2:] in nguyen_am_da:\n",
    "            res.append('o' + tieng[2:])\n",
    "        else:\n",
    "            res.append(tieng[2:])\n",
    "        return res\n",
    "    \n",
    "    # special case: âm[0] == y:\n",
    "    if tieng[0] == 'y':\n",
    "        if len(tieng) == 1:\n",
    "            res.append(tieng)\n",
    "        elif 'i' + tieng[1:] in nguyen_am_da:\n",
    "            res.append('i' + tieng[1:])\n",
    "        else:\n",
    "            print(tieng)\n",
    "            res.append(tieng)\n",
    "        return res\n",
    "        \n",
    "    for v in nguyen_am_da:\n",
    "        if v in tieng:\n",
    "            if len(v) > len(longest_v):\n",
    "                longest_v = v\n",
    "            res.append(v)\n",
    "    if longest_v != '':\n",
    "        res = [longest_v]\n",
    "    \n",
    "    if len(res) == 0:\n",
    "        for v_don in nguyen_am_don:\n",
    "            if v_don in tieng:\n",
    "                res.append(v_don)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_F</th>\n",
       "      <th>Type</th>\n",
       "      <th>Full_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABC</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Công ty Truyền thông ABC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADB</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Ngân hàng Phát triển châu Á</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADN</td>\n",
       "      <td>DNA</td>\n",
       "      <td>AND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANĐT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>an ninh điều tra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATGT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>An toàn Giao thông</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>XTTM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Xúc tiến thương mại</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Xét xử</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>YKVN</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Y khoa Việt Nam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>YTDP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y tế Dự phòng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>YTế</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y tế</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>876 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      C_F Type                    Full_Text\n",
       "0     ABC  ORG     Công ty Truyền thông ABC\n",
       "1     ADB  ORG  Ngân hàng Phát triển châu Á\n",
       "2     ADN  DNA                          AND\n",
       "3    ANĐT  NaN             an ninh điều tra\n",
       "4    ATGT  NaN           An toàn Giao thông\n",
       "..    ...  ...                          ...\n",
       "871  XTTM  NaN          Xúc tiến thương mại\n",
       "872    XX  NaN                       Xét xử\n",
       "873  YKVN  ORG              Y khoa Việt Nam\n",
       "874  YTDP  NaN                Y tế Dự phòng\n",
       "875   YTế  NaN                         Y tế\n",
       "\n",
       "[876 rows x 3 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vt_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cf_dict(csv_path):\n",
    "    vt_csv = pd.read_csv(csv_path, header = 0)\n",
    "    v_d = {}\n",
    "    for i in range(len(vt_csv)):\n",
    "        v_d[vt_csv.iloc[i, 0]] = vt_csv.iloc[i, 2]\n",
    "    return v_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_dict = create_cf_dict('Viet_tat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO(1):\n",
    "def replace_case_folding(am):\n",
    "    \"\"\"\n",
    "    GOAL: replace any c-f(viet tat) string in\n",
    "    a line with the full representation of that \n",
    "    phrase.\n",
    "    input: string (âm), string (csv file path for c-f dict)\n",
    "    returns: string (full_am)\n",
    "    \"\"\"\n",
    "    if am in cf_dict:\n",
    "        return cf_dict[am]\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(2)\n",
    "def lowercase_line(input_line):\n",
    "    \"\"\"\n",
    "    GOAL: lowercase every char in input_line\n",
    "    input: string\n",
    "    returns: string\n",
    "    \"\"\"\n",
    "    return input_line.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(3):\n",
    "def sub_special_chars_then_number(lwc_line):\n",
    "    \"\"\"\n",
    "    GOAL: Take in lower-cased line, sub special\n",
    "    chars with ' ', then sub nums with ' '.\n",
    "    input: string (lower-cased line)\n",
    "    returns: string (special chars and nums removed)\n",
    "    \"\"\"\n",
    "    rx_s_char = re.compile(r'[!@#$%\\^&*()[\\]{};:,./<>?|`~\\-=_+\\\\\"\\'\\“\\”\\…]')\n",
    "    rx_num = re.compile(r'[0-9]')\n",
    "    sc_subbed_line = re.sub(rx_s_char, ' ', lwc_line)\n",
    "    subbed_line = re.sub(rx_num, ' ', sc_subbed_line)\n",
    "    return subbed_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO():\n",
    "def split_line_to_am_list(cleaned_line):\n",
    "    \"\"\"\n",
    "    GOAL: split a lower-cased line into valid âm\n",
    "    input: string\n",
    "    output: list (of string)\n",
    "    \"\"\"\n",
    "    am_list = cleaned_line.split(' ')\n",
    "    return am_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_processed_tf(file_path):\n",
    "    \"\"\"\n",
    "    GOAL: \n",
    "    1) Normalization a text file (and save it \n",
    "    into a new file) for later steps.\n",
    "    input: raw text file (string)\n",
    "    returns: path of new processed text file (string)\n",
    "    \"\"\"\n",
    "    # create new empty .txt file:\n",
    "    processed_fname = 'processed_tf.txt'\n",
    "    processed_path = os.getcwd() + '/' + 'Processed Text File'\n",
    "    new_file_path = processed_path + '/' + processed_fname\n",
    "    with open(os.path.join(processed_path, processed_fname), 'w') as fp:\n",
    "        pass\n",
    "    \n",
    "    with open(file_path, mode = 'r') as rf, \\\n",
    "         open(new_file_path, mode = 'w') as wf:\n",
    "        for r_line in rf:\n",
    "            n_splitted = []\n",
    "            cf_split = sub_special_chars_then_number(r_line).split()\n",
    "            for am in cf_split:\n",
    "                \n",
    "                if am.isupper() == True:\n",
    "                    f_am = replace_case_folding(am)\n",
    "                    if f_am != None:\n",
    "                        n_splitted.append(f_am)\n",
    "                    else:\n",
    "                        n_splitted.append(am)\n",
    "                else:\n",
    "                    iy_normed = iy_normalization(am)\n",
    "                    am_and_thanh_num = normalize_am_with_thanh(iy_normed)\n",
    "                    n_splitted.append(am_and_thanh_num)\n",
    "                \n",
    "            n_splitted[-1] = n_splitted[-1] + '\\n'\n",
    "            low_r_line = lowercase_line(' '.join(n_splitted))\n",
    "            \n",
    "            wf.write(low_r_line)\n",
    "    \n",
    "    return new_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iy_normalization(am):\n",
    "    \"\"\"\n",
    "    GOAL: check if a given am needs\n",
    "    to be iy-normalized or not\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_am_with_thanh(am_with_thanh):\n",
    "    \"\"\"\n",
    "    GOAL: replace am_with_thanh with \n",
    "    am_and_thanh_key as a more standardized \n",
    "    format to run models.\n",
    "    \n",
    "    input: string (am_with_thanh)\n",
    "    returns: string (am_and_thanh_num)\n",
    "    Key: \n",
    "    Value: \n",
    "    convert âm_with_thanh -> am_and_thanh_num\n",
    "    eg: thuỷ -> thuy2\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING FUNCTION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_processed_tf(corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp = '/Users/trungdoquoc/Desktop/Personal Projects/VNese_assistant/Step_1_Preprocessing/Processed Text File/processed_tf.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "năm['ăm']\n",
      "ban['an']\n",
      "quyên['uyên']\n",
      "miên['iên']\n",
      "phi['i']\n",
      "phân['ân']\n",
      "mêm['êm']\n",
      "bao['ao']\n",
      "mât['ât']\n",
      "hang['ang']\n",
      "đâu['âu']\n",
      "thu['u']\n",
      "thuât['uât']\n",
      "dân['ân']\n",
      "tri['i']\n",
      "bao['ao']\n",
      "điên['iên']\n",
      "tư['ư']\n",
      "cua['ua']\n",
      "bao['ao']\n",
      "khuyên['uyên']\n",
      "hoc['oc']\n",
      "dân['ân']\n",
      "tri['i']\n",
      "giao['ao']\n",
      "diên['iên']\n",
      "pda['a']\n",
      "mua['ua']\n",
      "chung['ung']\n",
      "blog['o']\n",
      "tâm['âm']\n",
      "long['ong']\n",
      "nhân['ân']\n",
      "ai['ai']\n",
      "tuân['uân']\n",
      "bao['ao']\n",
      "mua['ua']\n",
      "ban['an']\n",
      "be['e']\n",
      "xinh['inh']\n",
      "diên['iên']\n",
      "đan['an']\n",
      "dân['ân']\n",
      "tri['i']\n",
      "english['eng']\n",
      "sư['ư']\n",
      "kiên['iên']\n",
      "xa['a']\n",
      "hôi['ôi']\n",
      "thê['ê']\n",
      "giơi['ơi']\n",
      "thê['ê']\n",
      "thao['ao']\n",
      "giao['ao']\n",
      "duc['uc']\n",
      "nhân['ân']\n",
      "ai['ai']\n",
      "kinh['inh']\n",
      "doanh['oanh']\n",
      "văn['ăn']\n",
      "hoa['oa']\n",
      "phap['ap']\n",
      "luât['uât']\n",
      "nhip['ip']\n",
      "sông['ông']\n",
      "tre['e']\n",
      "tinh['inh']\n",
      "yêu['iêu']\n",
      "sưc['ưc']\n",
      "khoe['oe']\n",
      "sưc['ưc']\n",
      "manh['anh']\n",
      "sô['ô']\n",
      "ô['ô']\n",
      "tô['ô']\n",
      "xe['e']\n",
      "may['ay']\n",
      "chuyên['uyên']\n",
      "la['a']\n",
      "ban['an']\n",
      "đoc['oc']\n",
      "vi['i']\n",
      "tinh['inh']\n",
      "điên['iên']\n",
      "thoai['oai']\n",
      "thu['u']\n",
      "thuât['uât']\n",
      "bom['om']\n",
      "tân['ân']\n",
      "tai['ai']\n",
      "hôi['ôi']\n",
      "nghi['i']\n",
      "di['i']\n",
      "đông['ông']\n",
      "thê['ê']\n",
      "giơi['ơi']\n",
      "mwc[]\n",
      "tranh['anh']\n",
      "châp['âp']\n",
      "ban['an']\n",
      "quyên['uyên']\n",
      "thương['ương']\n",
      "hiêu['iêu']\n",
      "ipad['ip']\n",
      "tai['ai']\n",
      "trung['ung']\n",
      "quôc['uôc']\n",
      "triên['iên']\n",
      "lam['am']\n",
      "ces['e']\n",
      "ceo['eo']\n",
      "facebook['ac']\n",
      "đên['ên']\n",
      "viêt['iêt']\n",
      "nam['am']\n",
      "thư['ư']\n",
      "bây['ây']\n",
      "năm['ăm']\n",
      "ban['an']\n",
      "quyên['uyên']\n",
      "miên['iên']\n",
      "phi['i']\n",
      "phân['ân']\n",
      "mêm['êm']\n",
      "bao['ao']\n",
      "mât['ât']\n",
      "hang['ang']\n",
      "đâu['âu']\n",
      "dân['ân']\n",
      "tri['i']\n",
      "bai['ai']\n",
      "viêt['iêt']\n",
      "dươi['ươi']\n",
      "đây['ây']\n",
      "se['e']\n",
      "giup['up']\n",
      "ban['an']\n",
      "tân['ân']\n",
      "dung['ung']\n",
      "cơ['ơ']\n",
      "hôi['ôi']\n",
      "đê['ê']\n",
      "sơ['ơ']\n",
      "hưu['ưu']\n",
      "ban['an']\n",
      "quyên['uyên']\n",
      "cua['ua']\n",
      "trend['en']\n",
      "micro['i', 'o']\n",
      "titanium['an']\n",
      "môt['ôt']\n",
      "trong['ong']\n",
      "nhưng['ưng']\n",
      "phân['ân']\n",
      "mêm['êm']\n",
      "bao['ao']\n",
      "mât['ât']\n",
      "manh['anh']\n",
      "me['e']\n",
      "nhât['ât']\n",
      "hiên['iên']\n",
      "nay['ay']\n",
      "vơi['ơi']\n",
      "han['an']\n",
      "dung['ung']\n",
      "lên['ên']\n",
      "đên['ên']\n",
      "tân['ân']\n",
      "năm\n",
      "['ăm']\n",
      "trend['en']\n",
      "micro['i', 'o']\n",
      "la['a']\n",
      "hang['ang']\n",
      "bao['ao']\n",
      "mât['ât']\n",
      "nhât['ât']\n",
      "ban['an']\n",
      "nôi['ôi']\n",
      "tiêng['iêng']\n",
      "vơi['ơi']\n",
      "nhưng['ưng']\n",
      "phân['ân']\n",
      "mêm['êm']\n",
      "bao['ao']\n",
      "vê['ê']\n",
      "may['ay']\n",
      "tinh['inh']\n",
      "manh['anh']\n",
      "me['e']\n",
      "va['a']\n",
      "hiêu['iêu']\n",
      "qua['ua']\n",
      "nôi['ôi']\n",
      "bât['ât']\n",
      "trong['ong']\n",
      "sô['ô']\n",
      "đo['o']\n",
      "la['a']\n",
      "trend['en']\n",
      "micro['i', 'o']\n",
      "titanium['an']\n",
      "môt['ôt']\n",
      "trong['ong']\n",
      "nhưng['ưng']\n",
      "phân['ân']\n",
      "mêm['êm']\n",
      "diêt['iêt']\n",
      "virus['i', 'u']\n",
      "manh['anh']\n",
      "me['e']\n",
      "va['a']\n",
      "hiêu['iêu']\n",
      "qua['ua']\n",
      "hang['ang']\n",
      "đâu['âu']\n",
      "hiên['iên']\n",
      "nay\n",
      "['ay']\n",
      "điêm['iêm']\n",
      "nôi['ôi']\n",
      "bât['ât']\n",
      "cua['ua']\n",
      "trend['en']\n",
      "micro['i', 'o']\n",
      "titanium['an']\n",
      "đo['o']\n",
      "la['a']\n",
      "kha['a']\n",
      "năng['ăng']\n",
      "hoat['oat']\n",
      "đông['ông']\n",
      "mươt['ươt']\n",
      "ma['a']\n",
      "trên['ên']\n",
      "ca['a']\n",
      "nhưng['ưng']\n",
      "hê['ê']\n",
      "thông['ông']\n",
      "co['o']\n",
      "câu['âu']\n",
      "hinh['inh']\n",
      "không['ông']\n",
      "qua['ua']\n",
      "manh\n",
      "['anh']\n",
      "trong['ong']\n",
      "qua['ua']\n",
      "trinh['inh']\n",
      "thư['ư']\n",
      "nghiêm['iêm']\n",
      "khi['i']\n",
      "tiên['iên']\n",
      "hanh['anh']\n",
      "quet['et']\n",
      "virus['i', 'u']\n",
      "phân['ân']\n",
      "mêm['êm']\n",
      "cung['ung']\n",
      "chi['i']\n",
      "chiêm['iêm']\n",
      "dung['ung']\n",
      "chưa['ưa']\n",
      "đên['ên']\n",
      "ngân['ân']\n",
      "hang['ang']\n",
      "tmcp[]\n",
      "quân['uân']\n",
      "đôi['ôi']\n",
      "dung['ung']\n",
      "lương['ương']\n",
      "cua['ua']\n",
      "bô['ô']\n",
      "nhơ['ơ']\n",
      "ram['am']\n",
      "môt['ôt']\n",
      "con['on']\n",
      "sô['ô']\n",
      "kha['a']\n",
      "ân['ân']\n",
      "tương\n",
      "['ương']\n",
      "ưu['ưu']\n",
      "điêm['iêm']\n",
      "thư['ư']\n",
      "cua['ua']\n",
      "phân['ân']\n",
      "mêm['êm']\n",
      "nay['ay']\n",
      "đo['o']\n",
      "la['a']\n",
      "kha['a']\n",
      "năng['ăng']\n",
      "sư['ư']\n",
      "dung['ung']\n",
      "dê['ê']\n",
      "dang['ang']\n",
      "vơi['ơi']\n",
      "giao['ao']\n",
      "diên['iên']\n",
      "đơn['ơn']\n",
      "gian['an']\n",
      "giup['up']\n",
      "ngươi['ươi']\n",
      "dung['ung']\n",
      "co['o']\n",
      "thê['ê']\n",
      "thao['ao']\n",
      "tac['ac']\n",
      "cac['ac']\n",
      "chưc['ưc']\n",
      "năng['ăng']\n",
      "môt['ôt']\n",
      "cach['ach']\n",
      "thuân['uân']\n",
      "tiên['iên']\n",
      "ngay['ay']\n",
      "ca['a']\n",
      "vơi['ơi']\n",
      "nhưng['ưng']\n",
      "ai['ai']\n",
      "không['ông']\n",
      "thanh['anh']\n",
      "thao['ao']\n",
      "vê['ê']\n",
      "may['ay']\n",
      "tinh\n",
      "['inh']\n",
      "giao['ao']\n",
      "diên['iên']\n",
      "trend['en']\n",
      "micro['i', 'o']\n",
      "titanium['an']\n",
      "rât['ât']\n",
      "đơn['ơn']\n",
      "gian['an']\n",
      "vơi['ơi']\n",
      "ưu['ưu']\n",
      "điêm['iêm']\n",
      "trên['ên']\n",
      "trend['en']\n",
      "micro['i', 'o']\n",
      "titanium['an']\n",
      "la['a']\n",
      "môt['ôt']\n",
      "trong['ong']\n",
      "nhưng['ưng']\n",
      "lưa['ưa']\n",
      "chon['on']\n",
      "phu['u']\n",
      "hơp['ơp']\n",
      "va['a']\n",
      "hoan['oan']\n",
      "hao['ao']\n",
      "nhât['ât']\n",
      "cho['o']\n",
      "vai['ai']\n",
      "tro['o']\n",
      "tâm['âm']\n",
      "la['a']\n",
      "chăn['ăn']\n",
      "bao['ao']\n",
      "vê['ê']\n",
      "an['an']\n",
      "toan['oan']\n",
      "cho['o']\n",
      "may['ay']\n",
      "tinh['inh']\n",
      "cua['ua']\n",
      "ban\n",
      "['an']\n",
      "môt['ôt']\n",
      "vai['ai']\n",
      "tinh['inh']\n",
      "năng['ăng']\n",
      "nôi['ôi']\n",
      "bât['ât']\n",
      "cua['ua']\n",
      "trend['en']\n",
      "micro['i', 'o']\n",
      "titanium['an']\n",
      "tim['im']\n",
      "va['a']\n",
      "diêt['iêt']\n",
      "virus['i', 'u']\n",
      "phân['ân']\n",
      "mêm['êm']\n",
      "gian['an']\n",
      "điêp['iêp']\n",
      "trojan['an']\n",
      "sâu['âu']\n",
      "may['ay']\n",
      "tinh['inh']\n",
      "va['a']\n",
      "cac['ac']\n",
      "loai['oai']\n",
      "phân['ân']\n",
      "mêm['êm']\n",
      "gây['ây']\n",
      "hai['ai']\n",
      "khac\n",
      "['ac']\n",
      "phat['at']\n",
      "hiên['iên']\n",
      "va['a']\n",
      "ngăn['ăn']\n",
      "chăn['ăn']\n",
      "cac['ac']\n",
      "loai['oai']\n",
      "phân['ân']\n",
      "mêm['êm']\n",
      "gia['a']\n",
      "mao['ao']\n",
      "xâm['âm']\n",
      "nhâp['âp']\n",
      "vao['ao']\n",
      "may['ay']\n",
      "tinh\n",
      "['inh']\n",
      "cơ['ơ']\n",
      "chê['ê']\n",
      "bao['ao']\n",
      "vê['ê']\n",
      "đê['ê']\n",
      "ngăn['ăn']\n",
      "chăn['ăn']\n",
      "may['ay']\n",
      "tinh['inh']\n",
      "bi['i']\n",
      "xâm['âm']\n",
      "nhâp['âp']\n",
      "va['a']\n",
      "bi['i']\n",
      "biên['iên']\n",
      "thanh['anh']\n",
      "may['ay']\n",
      "tinh['inh']\n",
      "ma['a']\n",
      "đê['ê']\n",
      "tham['am']\n",
      "gia['a']\n",
      "vao['ao']\n",
      "cac['ac']\n",
      "cuôc['uôc']\n",
      "tân['ân']\n",
      "công['ông']\n",
      "tư['ư']\n",
      "chôi['ôi']\n",
      "dich['ich']\n",
      "vu\n",
      "['u']\n",
      "phat['at']\n",
      "hiên['iên']\n",
      "va['a']\n",
      "ngăn['ăn']\n",
      "chăn['ăn']\n",
      "kip['ip']\n",
      "thơi['ơi']\n",
      "cac['ac']\n",
      "trang['ang']\n",
      "web['e']\n",
      "co['o']\n",
      "chưa['ưa']\n",
      "ma['a']\n",
      "đông['ông']\n",
      "hai['ai']\n",
      "đông['ông']\n",
      "thơi['ơi']\n",
      "se['e']\n",
      "ngăn['ăn']\n",
      "không['ông']\n",
      "cho['o']\n",
      "phep['ep']\n",
      "trinh['inh']\n",
      "duyêt['uyêt']\n",
      "web['e']\n",
      "thưc['ưc']\n",
      "thi['i']\n",
      "cac['ac']\n",
      "loai['oai']\n",
      "ma['a']\n",
      "đôc['ôc']\n",
      "hai['ai']\n",
      "nay['ay']\n",
      "nêu['êu']\n",
      "ngươi['ươi']\n",
      "dung['ung']\n",
      "không['ông']\n",
      "may['ay']\n",
      "truy['uy']\n",
      "câp['âp']\n",
      "nhâm['âm']\n",
      "vao['ao']\n",
      "chung\n",
      "['ung']\n"
     ]
    }
   ],
   "source": [
    "with open(cpp, mode = 'r') as rf:\n",
    "    i = 0\n",
    "    for line in rf:\n",
    "        if i < 10:\n",
    "            s_l = replace_thanh_of_tieng(split_line_to_am_list(line))\n",
    "            for am in s_l:\n",
    "                van = detect_van(am)\n",
    "                print(am + str(van))\n",
    "#                 if len() == 1:\n",
    "#                     print(detect_van(am))\n",
    "#                     if am[-1] == 'i' or am[-1] == 'y':\n",
    "#                         print(am)\n",
    "            i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ai']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_van('ai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
