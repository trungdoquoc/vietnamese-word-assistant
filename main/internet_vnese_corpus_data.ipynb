{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "corpus_path = '/Users/trungdoquoc/Desktop/Personal Projects/VNese_assistant/VNese_Corpus/small_corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nguyen_am_don = ['a', 'ă', 'â', 'e', 'ê', 'i', 'u', 'ư', 'o', 'ơ', 'ô', 'y']\n",
    "phu_am_db = ['gi', 'qu']\n",
    "\n",
    "thanh_dict = {\n",
    "    'sac': ['á', 'ắ', 'ấ', 'é', 'ế', 'í', 'ú', 'ứ', 'ó', 'ớ', 'ố', 'ý'],\n",
    "    'hoi': ['ả', 'ẳ', 'ẩ', 'ẻ', 'ể', 'ỉ', 'ủ', 'ử', 'ỏ', 'ở', 'ổ', 'ỷ'],\n",
    "    'nga': ['ã', 'ẵ', 'ẫ', 'ẽ', 'ễ', 'ĩ', 'ũ', 'ữ', 'õ', 'ỡ', 'ỗ', 'ỹ'],\n",
    "    'nang': ['ạ', 'ặ', 'ậ', 'ẹ', 'ệ', 'ị', 'ụ', 'ự', 'ọ', 'ợ', 'ộ', 'ỵ'],\n",
    "    'huyen': ['à', 'ằ', 'ầ', 'è', 'ề', 'ì', 'ù', 'ừ', 'ò', 'ờ', 'ồ', 'ỳ'],\n",
    "    'ngang': ['a', 'ă', 'â', 'e', 'ê', 'i', 'u', 'ư', 'o', 'ơ', 'ô', 'y']\n",
    "}\n",
    "\n",
    "bang_trac_dict = {\n",
    "    'T': ['sac', 'hoi', 'nga', 'nang'],\n",
    "    'B': ['huyen', 'ngang']\n",
    "}\n",
    "\n",
    "thanh_nguyen_am = {\n",
    "    'a': ['á', 'ả', 'ã', 'ạ', 'à', 'a'],\n",
    "    'ă': ['ắ', 'ẳ', 'ẵ', 'ặ', 'ằ', 'ă'],\n",
    "    'â': ['ấ', 'ẩ', 'ẫ', 'ậ', 'ầ', 'â'],\n",
    "    'e': ['é', 'ẻ', 'ẽ', 'ẹ', 'è', 'e'],\n",
    "    'ê': ['ế', 'ể', 'ễ', 'ệ', 'ề', 'ê'],\n",
    "    'i': ['í', 'ỉ', 'ĩ', 'ị', 'ì', 'i'],\n",
    "    'u': ['ú', 'ủ', 'ũ', 'ụ', 'ù', 'u'],\n",
    "    'ư': ['ứ', 'ử', 'ữ', 'ự', 'ừ', 'ư'],\n",
    "    'o': ['ó', 'ỏ', 'õ', 'ọ', 'ò', 'o'],\n",
    "    'ơ': ['ớ', 'ở', 'õ', 'ợ', 'ờ', 'ơ'],\n",
    "    'ô': ['ố', 'ổ', 'ỗ', 'ộ', 'ồ', 'ô'],\n",
    "    'y': ['ý', 'ỷ', 'ỹ', 'ỵ', 'ỳ', 'y']\n",
    "}\n",
    "\n",
    "###\n",
    "\n",
    "t1_vertical = ['a', 'ă', 'â', 'e', 'ê', 'i', 'o', 'ô', 'ơ', 'u', 'ư']\n",
    "t1_horizontal = ['a', 'c', 'ch', 'e/ê', 'i', 'm', 'n', 'ng', 'nh', 'o/ơ', 'p', 't', 'u', 'y']\n",
    "t1_matrix = [[0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "             [0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0],\n",
    "             [0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1],\n",
    "             [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0],\n",
    "             [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
    "             [1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
    "             [1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0],\n",
    "             [0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0],\n",
    "             [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0],\n",
    "             [1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1],\n",
    "             [1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0]]\n",
    "\n",
    "t2_vertical = ['iê', 'oa', 'oă', 'uâ', 'uô', 'uya', 'uyê', 'ươ', 'yê']\n",
    "t2_horizontal = ['a', 'c', 'ch', 'i', 'm', 'n', 'ng', 'nh', 'p', 't', 'u', 'y']\n",
    "t2_matrix = [[0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0],\n",
    "             [0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1],\n",
    "             [0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
    "             [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "             [0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0],\n",
    "             [1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
    "             [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "             [0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0],\n",
    "             [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0]]\n",
    "\n",
    "t1_complete = []\n",
    "for r in range(len(t1_matrix)):\n",
    "    row = []\n",
    "    for c in range(len(t1_matrix[r])):\n",
    "        if t1_matrix[r][c] == 1:\n",
    "            row.append(t1_vertical[r] + t1_horizontal[c])\n",
    "        else:\n",
    "            row.append('')\n",
    "    t1_complete.append(row)\n",
    "\n",
    "### Fix some special cases:\n",
    "t1_complete[6][3] = 'oe'\n",
    "t1_complete[9][3] = 'uê'\n",
    "t1_complete[0][9] = 'ao'\n",
    "t1_complete[3][9] = 'eo'\n",
    "t1_complete[9][9] = 'uơ'\n",
    "\n",
    "t2_complete = []\n",
    "for r in range(len(t2_matrix)):\n",
    "    row = []\n",
    "    for c in range(len(t2_matrix[r])):\n",
    "        if t2_matrix[r][c] == 1:\n",
    "            row.append(t2_vertical[r] + t2_horizontal[c])\n",
    "        else:\n",
    "            row.append('')\n",
    "    t2_complete.append(row)\n",
    "\n",
    "t2_complete[5][0] = 'uya'\n",
    "t2_complete[5][2] = 'uych'\n",
    "t2_complete[5][7] = 'uynh'\n",
    "t2_complete[5][9] = 'uyt'\n",
    "\n",
    "nguyen_am_da = []\n",
    "\n",
    "for r in t1_complete:\n",
    "    for e in r:\n",
    "        if e != '':\n",
    "            nguyen_am_da.append(e)\n",
    "            \n",
    "for r in t2_complete:\n",
    "    for e in r:\n",
    "        if e != '':\n",
    "            nguyen_am_da.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_thanh(tieng):\n",
    "    \"\"\"\n",
    "    Given a single tiếng, detect the thanh in it.\n",
    "    input: string\n",
    "    returns: tuples (B/T, thanh)\n",
    "    \n",
    "    HINT: a single tiếng always has a thanh, \n",
    "    if no thanh (other than ngang) is found, \n",
    "    then that tiếng has thanh ngang. Else, \n",
    "    thanh is the only thanh other than thanh ngang found.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for letter in tieng:\n",
    "        for k in thanh_dict:\n",
    "            if letter in thanh_dict[k]:\n",
    "                res.append(k)\n",
    "    for r in res:\n",
    "        if r != 'ngang':\n",
    "            for k in bang_trac_dict:\n",
    "                if r in bang_trac_dict[k]:\n",
    "                    return (k, r)\n",
    "    return ('B', 'ngang')\n",
    "\n",
    "def replace_thanh_of_tieng(am):\n",
    "    \"\"\"\n",
    "    For a single âm, detect the letter with thanh \n",
    "    of that âm, then replace it with that \n",
    "    letter without thanh.\n",
    "    input: string (a single am)\n",
    "    returns: string (âm without thanh)\n",
    "    \"\"\"\n",
    "    for c in am:\n",
    "        for k in thanh_nguyen_am:\n",
    "            if c in thanh_nguyen_am[k]:\n",
    "                am = am.replace(c, k)\n",
    "    return am\n",
    "\n",
    "def detect_van(tieng):\n",
    "    \"\"\"\n",
    "    Detect all possible vần in **tieng_ko_dau**\n",
    "    (both complete and partial vần)\n",
    "    input: a string\n",
    "    returns: a list of string\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    longest_v = ''\n",
    "    \n",
    "    # special case: van = gi:\n",
    "    # ng. am don:\n",
    "    if tieng[:2] == 'gi':\n",
    "        if tieng[2:] == '':\n",
    "            res.append('i')\n",
    "        elif tieng[2:] in nguyen_am_don or tieng[2:] in nguyen_am_da:\n",
    "            res.append(tieng[2:])\n",
    "        else:\n",
    "            res.append(tieng[1:])\n",
    "        return res\n",
    "        \n",
    "    # special case: van = qu:\n",
    "    if tieng[0] == 'q':\n",
    "        if tieng[1:] in nguyen_am_da:\n",
    "            res.append(tieng[1:])\n",
    "        elif 'o' + tieng[2:] in nguyen_am_da:\n",
    "            res.append('o' + tieng[2:])\n",
    "        else:\n",
    "            res.append(tieng[2:])\n",
    "        return res\n",
    "    \n",
    "    # special case: âm[0] == y:\n",
    "    if tieng[0] == 'y':\n",
    "        if len(tieng) == 1:\n",
    "            res.append(tieng)\n",
    "        elif 'i' + tieng[1:] in nguyen_am_da:\n",
    "            res.append('i' + tieng[1:])\n",
    "        else:\n",
    "            print(tieng)\n",
    "            res.append(tieng)\n",
    "        return res\n",
    "        \n",
    "    for v in nguyen_am_da:\n",
    "        if v in tieng:\n",
    "            if len(v) > len(longest_v):\n",
    "                longest_v = v\n",
    "            res.append(v)\n",
    "    if longest_v != '':\n",
    "        res = [longest_v]\n",
    "    \n",
    "    if len(res) == 0:\n",
    "        for v_don in nguyen_am_don:\n",
    "            if v_don in tieng:\n",
    "                res.append(v_don)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cf_dict(csv_path):\n",
    "    vt_csv = pd.read_csv(csv_path, header = 0)\n",
    "    v_d = {}\n",
    "    for i in range(len(vt_csv)):\n",
    "        v_d[vt_csv.iloc[i, 0]] = vt_csv.iloc[i, 2]\n",
    "    return v_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_dict = create_cf_dict('Viet_tat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO(1):\n",
    "def replace_case_folding(am):\n",
    "    \"\"\"\n",
    "    GOAL: replace any c-f(viet tat) string in\n",
    "    a line with the full representation of that \n",
    "    phrase.\n",
    "    input: string (âm), string (csv file path for c-f dict)\n",
    "    returns: string (full_am)\n",
    "    \"\"\"\n",
    "    if am in cf_dict:\n",
    "        return cf_dict[am]\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(2)\n",
    "def lowercase_line(input_line):\n",
    "    \"\"\"\n",
    "    GOAL: lowercase every char in input_line\n",
    "    input: string\n",
    "    returns: string\n",
    "    \"\"\"\n",
    "    return input_line.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(3):\n",
    "def sub_special_chars_then_number(lwc_line):\n",
    "    \"\"\"\n",
    "    GOAL: Take in lower-cased line, sub special\n",
    "    chars with ' ', then sub nums with ' '.\n",
    "    input: string (lower-cased line)\n",
    "    returns: string (special chars and nums removed)\n",
    "    \"\"\"\n",
    "    rx_s_char = re.compile(r'[!@#$%\\^&*()[\\]{};:,./<>?|`~\\-=_+\\\\\"\\'\\“\\”\\…]')\n",
    "    rx_num = re.compile(r'[0-9]')\n",
    "    sc_subbed_line = re.sub(rx_s_char, ' ', lwc_line)\n",
    "    subbed_line = re.sub(rx_num, ' ', sc_subbed_line)\n",
    "    return subbed_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO():\n",
    "def split_line_to_am_list(cleaned_line):\n",
    "    \"\"\"\n",
    "    GOAL: split a lower-cased line into valid âm\n",
    "    input: string\n",
    "    output: list (of string)\n",
    "    \"\"\"\n",
    "    am_list = cleaned_line.split(' ')\n",
    "    return am_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_processed_tf(file_path):\n",
    "    \"\"\"\n",
    "    GOAL: \n",
    "    1) Normalization a text file (and save it \n",
    "    into a new file) for later steps.\n",
    "    input: raw text file (string)\n",
    "    returns: path of new processed text file (string)\n",
    "    \"\"\"\n",
    "    # create new empty .txt file:\n",
    "    processed_fname = 'processed_tf.txt'\n",
    "    processed_path = os.getcwd() + '/' + 'Processed Text File'\n",
    "    new_file_path = processed_path + '/' + processed_fname\n",
    "    with open(os.path.join(processed_path, processed_fname), 'w') as fp:\n",
    "        pass\n",
    "    \n",
    "    with open(file_path, mode = 'r') as rf, \\\n",
    "         open(new_file_path, mode = 'w') as wf:\n",
    "        for r_line in rf:\n",
    "            n_splitted = []\n",
    "            cf_split = sub_special_chars_then_number(r_line).split()\n",
    "            for am in cf_split:\n",
    "                \n",
    "                # Check if am is viet_tat (Case_folding):\n",
    "                if am.isupper() == True:\n",
    "                    f_am = replace_case_folding(am)\n",
    "                    if f_am != None:\n",
    "                        n_splitted.append(f_am)\n",
    "                    else:\n",
    "                        n_splitted.append(am)\n",
    "                \n",
    "                else:\n",
    "#                     iy_normed = iy_normalization(am)\n",
    "                    am_and_thanh_num = normalize_am_with_thanh(am)\n",
    "                    n_splitted.append(am_and_thanh_num)\n",
    "                \n",
    "            n_splitted[-1] = n_splitted[-1] + '\\n'\n",
    "            low_r_line = lowercase_line(' '.join(n_splitted))\n",
    "            \n",
    "            wf.write(low_r_line)\n",
    "    \n",
    "    return new_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iy_normalization(am):\n",
    "    \"\"\"\n",
    "    GOAL: check if a given am needs\n",
    "    to be iy-normalized or not\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_am_with_thanh(am_with_thanh):\n",
    "    \"\"\"\n",
    "    GOAL: replace am_with_thanh with \n",
    "    am_and_thanh_key as a more standardized \n",
    "    format to run models.\n",
    "    \n",
    "    input: string (am_with_thanh)\n",
    "    returns: string (am_and_thanh_num)\n",
    "    Key: \n",
    "    Value: \n",
    "    convert âm_with_thanh -> am_and_thanh_num\n",
    "    eg: thuỷ -> thuy1\n",
    "    \"\"\"\n",
    "    thanh_dieu = ['sac', 'hoi', 'nga', 'nang', 'huyen', 'ngang']\n",
    "    thanh_num = thanh_dieu.index(detect_thanh(am_with_thanh)[1])\n",
    "    am = replace_thanh_of_tieng(am_with_thanh)\n",
    "    if thanh_num != 5:\n",
    "        return am + str(thanh_num)\n",
    "    else:\n",
    "        return am"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING FUNCTION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/trungdoquoc/Desktop/Personal Projects/VNese_assistant/Step_1_Preprocessing/Processed Text File/processed_tf.txt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_new_processed_tf(corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "original_file = '/Users/trungdoquoc/Desktop/Personal Projects/VNese_assistant/VNese_Corpus/small_corpus.txt'\n",
    "cpp = '/Users/trungdoquoc/Desktop/Personal Projects/VNese_assistant/Step_1_Preprocessing/Processed Text File/processed_tf.txt'\n",
    "\n",
    "n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cpp, mode = 'r') as rf:\n",
    "    i = 0\n",
    "    g_dict = {}\n",
    "    for line in rf:\n",
    "        if i < 1000:\n",
    "            bigrams = ngrams(line.split(), n)\n",
    "            for grams in bigrams:\n",
    "                text_gram = grams[0] + ' ' + grams[1]\n",
    "                if text_gram in g_dict:\n",
    "                    g_dict[text_gram] += 1\n",
    "                else:\n",
    "                    g_dict[text_gram] = 1\n",
    "#                 if len() == 1:\n",
    "#                     print(detect_van(am))\n",
    "#                     if am[-1] == 'i' or am[-1] == 'y':\n",
    "#                         print(am)\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_g = sorted(g_dict.items(), key=lambda x: int(x[1]), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
