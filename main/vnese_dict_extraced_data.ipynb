{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem 1: words extraction from Vietnamese dictionary\n",
    "- extract words (từ), types (loại từ), meaning (nghĩa),  theme tag (chủ đề)* later\n",
    "- create pandas Dataframe to store the data.\n",
    "\n",
    "problem 2: words deconstruction\n",
    "- deconstruct words into parts (âm đầu, vần, thanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_fp = '/Users/trungdoquoc/Desktop/Personal Projects/VNese_assistant/VV/vv30K.dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "nguyen_am_don = ['a', 'ă', 'â', 'e', 'ê', 'i', 'u', 'ư', 'o', 'ơ', 'ô', 'y']\n",
    "phu_am_db = ['gi', 'qu']\n",
    "\n",
    "thanh_dict = {\n",
    "    'sac': ['á', 'ắ', 'ấ', 'é', 'ế', 'í', 'ú', 'ứ', 'ó', 'ớ', 'ố', 'ý'],\n",
    "    'hoi': ['ả', 'ẳ', 'ẩ', 'ẻ', 'ể', 'ỉ', 'ủ', 'ử', 'ỏ', 'ở', 'ổ', 'ỷ'],\n",
    "    'nga': ['ã', 'ẵ', 'ẫ', 'ẽ', 'ễ', 'ĩ', 'ũ', 'ữ', 'õ', 'ỡ', 'ỗ', 'ỹ'],\n",
    "    'nang': ['ạ', 'ặ', 'ậ', 'ẹ', 'ệ', 'ị', 'ụ', 'ự', 'ọ', 'ợ', 'ộ', 'ỵ'],\n",
    "    'huyen': ['à', 'ằ', 'ầ', 'è', 'ề', 'ì', 'ù', 'ừ', 'ò', 'ờ', 'ồ', 'ỳ'],\n",
    "    'ngang': ['a', 'ă', 'â', 'e', 'ê', 'i', 'u', 'ư', 'o', 'ơ', 'ô', 'y']\n",
    "}\n",
    "\n",
    "bang_trac_dict = {\n",
    "    'T': ['sac', 'hoi', 'nga', 'nang'],\n",
    "    'B': ['huyen', 'ngang']\n",
    "}\n",
    "\n",
    "thanh_nguyen_am = {\n",
    "    'a': ['á', 'ả', 'ã', 'ạ', 'à', 'a'],\n",
    "    'ă': ['ắ', 'ẳ', 'ẵ', 'ặ', 'ằ', 'ă'],\n",
    "    'â': ['ấ', 'ẩ', 'ẫ', 'ậ', 'ầ', 'â'],\n",
    "    'e': ['é', 'ẻ', 'ẽ', 'ẹ', 'è', 'e'],\n",
    "    'ê': ['ế', 'ể', 'ễ', 'ệ', 'ề', 'ê'],\n",
    "    'i': ['í', 'ỉ', 'ĩ', 'ị', 'ì', 'i'],\n",
    "    'u': ['ú', 'ủ', 'ũ', 'ụ', 'ù', 'u'],\n",
    "    'ư': ['ứ', 'ử', 'ữ', 'ự', 'ừ', 'ư'],\n",
    "    'o': ['ó', 'ỏ', 'õ', 'ọ', 'ò', 'o'],\n",
    "    'ơ': ['ớ', 'ở', 'õ', 'ợ', 'ờ', 'ơ'],\n",
    "    'ô': ['ố', 'ổ', 'ỗ', 'ộ', 'ồ', 'ô'],\n",
    "    'y': ['ý', 'ỷ', 'ỹ', 'ỵ', 'ỳ', 'y']\n",
    "}\n",
    "\n",
    "###\n",
    "\n",
    "t1_vertical = ['a', 'ă', 'â', 'e', 'ê', 'i', 'o', 'ô', 'ơ', 'u', 'ư']\n",
    "t1_horizontal = ['a', 'c', 'ch', 'e/ê', 'i', 'm', 'n', 'ng', 'nh', 'o/ơ', 'p', 't', 'u', 'y']\n",
    "t1_matrix = [[0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "             [0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0],\n",
    "             [0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1],\n",
    "             [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0],\n",
    "             [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
    "             [1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
    "             [1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0],\n",
    "             [0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0],\n",
    "             [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0],\n",
    "             [1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1],\n",
    "             [1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0]]\n",
    "\n",
    "t2_vertical = ['iê', 'oa', 'oă', 'uâ', 'uô', 'uya', 'uyê', 'ươ', 'yê']\n",
    "t2_horizontal = ['a', 'c', 'ch', 'i', 'm', 'n', 'ng', 'nh', 'p', 't', 'u', 'y']\n",
    "t2_matrix = [[0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0],\n",
    "             [0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1],\n",
    "             [0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
    "             [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "             [0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0],\n",
    "             [1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
    "             [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "             [0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0],\n",
    "             [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0]]\n",
    "\n",
    "t1_complete = []\n",
    "for r in range(len(t1_matrix)):\n",
    "    row = []\n",
    "    for c in range(len(t1_matrix[r])):\n",
    "        if t1_matrix[r][c] == 1:\n",
    "            row.append(t1_vertical[r] + t1_horizontal[c])\n",
    "        else:\n",
    "            row.append('')\n",
    "    t1_complete.append(row)\n",
    "\n",
    "### Fix some special cases:\n",
    "t1_complete[6][3] = 'oe'\n",
    "t1_complete[9][3] = 'uê'\n",
    "t1_complete[0][9] = 'ao'\n",
    "t1_complete[3][9] = 'eo'\n",
    "t1_complete[9][9] = 'uơ'\n",
    "\n",
    "t2_complete = []\n",
    "for r in range(len(t2_matrix)):\n",
    "    row = []\n",
    "    for c in range(len(t2_matrix[r])):\n",
    "        if t2_matrix[r][c] == 1:\n",
    "            row.append(t2_vertical[r] + t2_horizontal[c])\n",
    "        else:\n",
    "            row.append('')\n",
    "    t2_complete.append(row)\n",
    "\n",
    "t2_complete[5][0] = 'uya'\n",
    "t2_complete[5][2] = 'uych'\n",
    "t2_complete[5][7] = 'uynh'\n",
    "t2_complete[5][9] = 'uyt'\n",
    "\n",
    "nguyen_am_da = []\n",
    "\n",
    "for r in t1_complete:\n",
    "    for e in r:\n",
    "        if e != '':\n",
    "            nguyen_am_da.append(e)\n",
    "            \n",
    "for r in t2_complete:\n",
    "    for e in r:\n",
    "        if e != '':\n",
    "            nguyen_am_da.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting each word definition (word + meaning) as an element of a list\n",
    "#input: file_path --> output: 2-D list\n",
    "def get_word_definition(file_path):\n",
    "    all_words = [] # 2-D list, each element is (word + meaning)\n",
    "    with open(file_path) as f:\n",
    "        new_w = []\n",
    "        for line in f:\n",
    "            if line[0] == '@':\n",
    "                new_w.append(line.strip('@').strip('\\n'))\n",
    "            elif line == '\\n':\n",
    "                all_words.append(new_w)\n",
    "                new_w = []\n",
    "            else:\n",
    "                new_w.append(line.strip('\\n'))\n",
    "        all_words.append(new_w) # append the last word in the dictionary\n",
    "    return all_words\n",
    "\n",
    "# create a dict, key: the word, value: a list of type\n",
    "# input: 2-D word_list --> output: word dict\n",
    "def create_dict_from_wlist(word_list):\n",
    "    new_dict = {}\n",
    "    for word in word_list:\n",
    "        meaning = ''\n",
    "        for e in range(1, len(word)):\n",
    "            if e != len(word)-1:\n",
    "                meaning = meaning + word[e] + '\\n'\n",
    "            else: \n",
    "                meaning = meaning + word[e]\n",
    "        new_dict[word[0]] = meaning\n",
    "    return new_dict\n",
    "\n",
    "#create a dict, key: the word, value: \n",
    "\n",
    "def create_list_of_dict(word_dict):\n",
    "    new_list = []\n",
    "    for d in word_dict:\n",
    "        s_dict = {}\n",
    "        s_dict[d] = word_dict.get(d)\n",
    "        new_list.append(s_dict)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_1 = get_word_definition(full_fp)\n",
    "all_dict = create_dict_from_wlist(all_words_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Push all (word+meaning) into Dataframe\n",
    "def create_new_word_df(word_dict):\n",
    "    word_df = pd.DataFrame.from_dict(data = word_dict, orient = 'index')\n",
    "    word_df.columns = ['Word_Meaning']\n",
    "    word_df.index.name = 'Word'\n",
    "\n",
    "    # Số lượng tiếng\n",
    "    word_df['Số_lượng_tiếng'] = ''\n",
    "    for w in word_df.index:\n",
    "        n_w = w.replace('-', ' ')\n",
    "        l = len(n_w.split())\n",
    "        word_df.at[w, 'Số_lượng_tiếng'] = l\n",
    "\n",
    "    #New column for Thanh and Van\n",
    "    df_thanh_cols = ['Th1', 'Th2', 'Th3', 'Th4']\n",
    "    df_van_cols = ['T1', 'T2', 'T3', 'T4']\n",
    "\n",
    "    for c in df_thanh_cols:\n",
    "        word_df[c] = ''\n",
    "\n",
    "    for c in df_van_cols:\n",
    "        word_df[c] = ''\n",
    "\n",
    "    valid_word = word_df.loc[word_df['Số_lượng_tiếng'] <= 4]\n",
    "    return valid_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_d = valid_word.sort_values(by = 'Số_lượng_tiếng', ascending = True)[-440:-390]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Part 1: Detect thanh in a word -- \n",
    "def split_word_to_tieng(word):\n",
    "    \"\"\"\n",
    "    take a word, split it as a list of tieng\n",
    "    input: a string of word\n",
    "    returns: a list of 'clean' tieng\n",
    "    \"\"\"\n",
    "    char_list = ['.', ',']\n",
    "    tieng_list = word.lower().replace('-', ' ').replace('.', ' ').replace(',', ' ').split()\n",
    "    return tieng_list\n",
    "\n",
    "def detect_thanh(tieng):\n",
    "    \"\"\"\n",
    "    Given a single tiếng, detect the thanh in it.\n",
    "    input: string | output: string (B or T)\n",
    "    \n",
    "    HINT: a single tiếng always has a thanh, \n",
    "    if no thanh (other than ngang) is found, \n",
    "    then that tiếng has thanh ngang. Else, \n",
    "    thanh is the only thanh other than thanh ngang found.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for letter in tieng:\n",
    "        for k in thanh_dict:\n",
    "            if letter in thanh_dict[k]:\n",
    "                res.append(k)\n",
    "    for r in res:\n",
    "        if r != 'ngang':\n",
    "            for k in bang_trac_dict:\n",
    "                if r in bang_trac_dict[k]:\n",
    "                    return k\n",
    "    return 'B'\n",
    "        \n",
    "# -- Part 2: Detect vần in a word -- \n",
    "def replace_thanh_of_tieng(list_tieng_w_thanh):\n",
    "    \"\"\"\n",
    "    For a list of tieng, detect the letter with thanh \n",
    "    in each tiếng element, then replace it with that l\n",
    "    etter without thanh.\n",
    "    input: a list of tiếng in a word\n",
    "    returns: a list of tiếng, each tiếng is without thanh\n",
    "    \"\"\"\n",
    "    list_tieng_wo_thanh = [''] * len(list_tieng_w_thanh)\n",
    "    \n",
    "    for t in range(len(list_tieng_w_thanh)):\n",
    "        tieng = list_tieng_w_thanh[t]\n",
    "        for c in tieng:\n",
    "            for k in thanh_nguyen_am:\n",
    "                if c in thanh_nguyen_am[k]:\n",
    "                    tieng = tieng.replace(c, k)\n",
    "        list_tieng_wo_thanh[t] = tieng\n",
    "    return list_tieng_wo_thanh\n",
    "\n",
    "def detect_van(tieng):\n",
    "    \"\"\"\n",
    "    Detect all possible vần in **tieng_ko_dau**\n",
    "    (both complete and partial vần)\n",
    "    input: a string\n",
    "    returns: a list of string\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    longest_v = ''\n",
    "    \n",
    "    # special case: van = gi:\n",
    "    # ng. am don:\n",
    "    if tieng[:2] == 'gi':\n",
    "        if tieng[2:] == '':\n",
    "            res.append('i')\n",
    "        elif tieng[2:] in nguyen_am_don or tieng[2:] in nguyen_am_da:\n",
    "            res.append(tieng[2:])\n",
    "        else:\n",
    "            res.append(tieng[1:])\n",
    "        return res\n",
    "        \n",
    "    # special case: van = qu:\n",
    "    if tieng[0] == 'q':\n",
    "        if tieng[1:] in nguyen_am_da:\n",
    "            res.append(tieng[1:])\n",
    "        elif 'o' + tieng[2:] in nguyen_am_da:\n",
    "            res.append('o' + tieng[2:])\n",
    "        else:\n",
    "            res.append(tieng[2:])\n",
    "        return res\n",
    "    \n",
    "    # special case: âm[0] == y:\n",
    "    if tieng[0] == 'y':\n",
    "        if len(tieng) == 1:\n",
    "            res.append(tieng)\n",
    "        elif 'i' + tieng[1:] in nguyen_am_da:\n",
    "            res.append('i' + tieng[1:])\n",
    "        else:\n",
    "            print(tieng)\n",
    "            res.append(tieng)\n",
    "        return res\n",
    "        \n",
    "    for v in nguyen_am_da:\n",
    "        if v in tieng:\n",
    "            if len(v) > len(longest_v):\n",
    "                longest_v = v\n",
    "            res.append(v)\n",
    "    if longest_v != '':\n",
    "        res = [longest_v]\n",
    "    \n",
    "    if len(res) == 0:\n",
    "        for v_don in nguyen_am_don:\n",
    "            if v_don in tieng:\n",
    "                res.append(v_don)\n",
    "    return res\n",
    "        \n",
    "def add_thanh_and_van_to_word_index(word_dataframe):\n",
    "    for w in word_dataframe.index:\n",
    "        formatted_w_1 = split_word_to_tieng(w) #split into a list of tiếng\n",
    "        formatted_w_2 = replace_thanh_of_tieng(split_word_to_tieng(w))\n",
    "        \n",
    "        if len(formatted_w_1) != len(formatted_w_2):\n",
    "            print(\"ERROR ERROR!\")\n",
    "        else:\n",
    "            while len(formatted_w_1) < 4:\n",
    "                formatted_w_1.append('')\n",
    "\n",
    "            while len(formatted_w_2) < 4:\n",
    "                formatted_w_2.append('')\n",
    "\n",
    "            thanh_res = [''] * len(formatted_w_1)\n",
    "            van_res = [''] * len(formatted_w_2)\n",
    "\n",
    "            for t in range(len(formatted_w_1)):\n",
    "                if formatted_w_1[t] != '' and formatted_w_2[t] != '':\n",
    "                    thanh_res[t] = detect_thanh(formatted_w_1[t])\n",
    "                    van_res[t] = detect_van(formatted_w_2[t])\n",
    "\n",
    "            for c in range(len(df_thanh_cols)):\n",
    "                word_dataframe.at[w, df_thanh_cols[c]] = thanh_res[c]\n",
    "\n",
    "            for c in range(len(df_van_cols)):\n",
    "                word_dataframe.at[w, df_van_cols[c]] = van_res[c]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
